#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""!
@file pic.flow
@brief A comprehensive conductor script for the PIC-Flow simulation platform.

This script acts as the central user interface for running simulations,
managing configurations, and orchestrating the entire end-to-end workflow.
It translates user-friendly YAML files into C-solver compatible control files,
supports full multi-block configurations, and provides live log streaming.
It features intelligent, content-based config file discovery and robustly
manages data I/O paths for the post-processor.
"""

import yaml
import sys
import os
import argparse
import subprocess
import shutil
import glob
import numpy as np
from datetime import datetime

# --- Global Path Definitions ---
SCRIPT_PATH = os.path.dirname(os.path.realpath(__file__))
BIN_DIR = SCRIPT_PATH
PROJECT_ROOT = os.path.dirname(BIN_DIR)

# ==============================================================================
# HELPER FUNCTIONS
# ==============================================================================

def read_yaml_file(filepath: str) -> dict:
    """!
    @brief Safely reads a YAML file and returns its content.
    @param[in] filepath Path to the YAML file.
    @return A dictionary containing the parsed YAML content.
    @throws SystemExit if the file is not found or cannot be parsed.
    """
    if not os.path.exists(filepath):
        print(f"[FATAL] Configuration file not found: {filepath}", file=sys.stderr)
        sys.exit(1)
    try:
        with open(filepath, 'r') as f:
            return yaml.safe_load(f)
    except yaml.YAMLError as e:
        print(f"[FATAL] Error parsing YAML file '{filepath}': {e}", file=sys.stderr)
        sys.exit(1)

def generate_header(run_id: str, source_files: dict) -> str:
    """!
    @brief Creates a standard header block for all generated files.
    @param[in] run_id The unique identifier for the current simulation run.
    @param[in] source_files A dictionary of source profile files used.
    @return A formatted string containing the header.
    """
    header_parts = [
        "# ==============================================================================",
        "#                AUTO-GENERATED CONFIGURATION FILE",
        "# ------------------------------------------------------------------------------",
        f"#   Run ID:       {run_id}",
        f"#   Timestamp:    {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        "#",
        "#   Source Configuration:"
    ]
    for name, path in source_files.items():
        header_parts.append(f"#     - {name:<12}: {os.path.basename(path)}")
    header_parts.extend([
        "#",
        "#   DO NOT EDIT THIS FILE MANUALLY. IT IS A MACHINE-READABLE ARTIFACT.",
        "# ==============================================================================\n"
    ])
    return "\n".join(header_parts)

def generate_simple_list_file(run_dir: str, run_id: str, cfg: dict, section: str, key: str, filename: str, header_sources: dict) -> str:
    """!
    @brief Generic function to create a file containing a simple list of strings.
    @param[in] run_dir The path to the main run directory.
    @param[in] run_id The unique identifier for the run.
    @param[in] cfg The dictionary containing the configuration data.
    @param[in] section The top-level key in the cfg dictionary.
    @param[in] key The second-level key whose value is the list of strings.
    @param[in] filename The name of the file to generate (e.g., 'whitelist.run').
    @param[in] header_sources A dictionary of source files for the header.
    @return The absolute path to the generated file.
    """
    print(f"[INFO] Generating {filename}...")
    config_dir = os.path.join(run_dir, "config")
    file_path = os.path.join(config_dir, filename)
    
    lines = [generate_header(run_id, header_sources)]
    items = cfg.get(section, {}).get(key, [])
    lines.extend(items)

    with open(file_path, "w") as f: f.write("\n".join(lines))
    print(f"[SUCCESS] Generated {filename}: {os.path.relpath(file_path)}")
    return os.path.abspath(file_path)

def generate_multi_block_bcs(run_dir: str, run_id: str, case_cfg: dict, source_files: dict) -> list:
    """!
    @brief Parses multi-block BCs from YAML, generates a .run file for each block,
           and returns a list of their absolute paths.
    @details Handles both simple list format (for single-block cases) and a
             list-of-lists (for multi-block cases) for boundary conditions.
    @param[in] run_dir The path to the main run directory.
    @param[in] run_id The unique identifier for the run.
    @param[in] case_cfg The parsed case.yml configuration dictionary.
    @param[in] source_files A dictionary of source files for the header.
    @return A list of absolute paths to the generated BC files.
    @throws ValueError if the number of BC definitions does not match the number of blocks.
    """
    print("[INFO] Generating boundary condition files...")
    config_dir = os.path.join(run_dir, "config")
    
    all_blocks_bcs = case_cfg.get('boundary_conditions', [])
    num_blocks = case_cfg.get('models', {}).get('domain', {}).get('blocks', 1)

    if not all_blocks_bcs:
        raise ValueError("The 'boundary_conditions' section in case.yml is empty.")

    is_simple_list = isinstance(all_blocks_bcs[0], dict)
    if num_blocks == 1 and is_simple_list:
        print("[INFO] Detected simple list format for single-block case. Wrapping automatically.")
        all_blocks_bcs = [all_blocks_bcs]

    if len(all_blocks_bcs) != num_blocks:
        raise ValueError(
            f"Mismatch: case.yml declares {num_blocks} block(s) but found {len(all_blocks_bcs)} BC definitions."
        )

    generated_files = []
    for i, block_bcs_list in enumerate(all_blocks_bcs):
        bcs_file_path = os.path.join(config_dir, f"bcs_block{i}.run")
        bcs_lines = [generate_header(run_id, source_files)]

        for bc in block_bcs_list:
            face, bc_type, handler = bc['face'], bc['type'], bc['handler']
            params_str = ""
            if bc.get('params'):
                parts = [f"{k}={','.join(map(str,v)) if isinstance(v,list) else v}" for k, v in bc['params'].items()]
                params_str = " ".join(parts)
            bcs_lines.append(f"{face:<20s} {bc_type:<12s} {handler:<20s} {params_str}")
        
        with open(bcs_file_path, "w") as f: f.write("\n".join(bcs_lines))
        
        print(f"[SUCCESS] Generated BCs for Block {i}: {os.path.relpath(bcs_file_path)}")
        generated_files.append(os.path.abspath(bcs_file_path))
        
    return generated_files

def format_flag_value(value):
    """!
    @brief Converts Python types to C-style command-line flag values.
    @param[in] value The Python object to convert (bool, list, or other).
    @return A string representation suitable for a C command-line parser.
    """
    if isinstance(value, bool):
        return "1" if value else "0"
    if isinstance(value, list):
        return ",".join(map(str, value))
    return str(value)

def parse_and_add_model_flags(case_cfg: dict, control_lines: list):
    """!
    @brief Parses the 'models' section of case.yml and adds corresponding C-solver flags.
    @param[in] case_cfg The parsed case.yml configuration dictionary.
    @param[out] control_lines A list of strings to which C-flags will be appended.
    """
    models = case_cfg.get('models', {})
    FLAG_MAP = {
        'domain': {'blocks': '-nblk', 'i_periodic': '-i_periodic', 'j_periodic': '-j_periodic', 'k_periodic': '-k_periodic'},
        'physics.fsi': {'immersed': '-imm', 'moving_fsi': '-fsi'},
        'physics.particles': {'count': '-numParticles'},
        'physics.turbulence': {'les': '-les', 'rans': '-rans', 'wall_function': '-wallfunction'},
        'statistics': {'time_averaging': '-averaging'}
    }
    for section_path, flags in FLAG_MAP.items():
        current_level = models
        try:
            for key in section_path.split('.'): current_level = current_level[key]
            for yaml_key, flag in flags.items():
                if yaml_key in current_level:
                    control_lines.append(f"{flag} {format_flag_value(current_level[yaml_key])}")
        except KeyError: continue

    if models.get('physics', {}).get('dimensionality') == '2D':
        control_lines.append("-TwoD 1")
    
    p_init_map = {'Surface': 0, 'Volume': 1}
    p_init_mode_str = models.get('physics', {}).get('particles', {}).get('init_mode', 'Surface')
    pinit_code = p_init_map.get(p_init_mode_str)
    if pinit_code is None:
        raise ValueError(f"Unknown particle init_mode '{p_init_mode_str}'. Options are {list(p_init_map.keys())}")
    control_lines.append(f"-pinit {pinit_code}")
    print(f"  - Particle Initialization Mode: {p_init_mode_str} (Code: {pinit_code})")
    
    p_restart_mode = models.get('physics', {}).get('particles', {}).get('restart_mode')
    if p_restart_mode:
        control_lines.append(f"-particle_restart_mode \"{p_restart_mode}\"")

def parse_solver_config(solver_cfg: dict) -> dict:
    """!
    @brief Parses the structured solver.yml into a flat dictionary of {flag: value}.
    @param[in] solver_cfg The parsed solver.yml configuration dictionary.
    @return A dictionary where keys are C-solver flags and values are the corresponding settings.
    """
    flags = {}
    if 'operation_mode' in solver_cfg and 'eulerian_field_source' in solver_cfg['operation_mode']:
        flags['-euler_field_source'] = f"\"{solver_cfg['operation_mode'].get('eulerian_field_source', 'solve')}\""
    if 'strategy' in solver_cfg:
        s = solver_cfg['strategy']
        if 'implicit' in s: flags['-imp'] = format_flag_value(s['implicit'])
        if 'central_diff' in s: flags['-central'] = format_flag_value(s['central_diff'])
    if 'tolerances' in solver_cfg:
        t = solver_cfg['tolerances']
        tol_map = {'max_iterations': '-imp_MAX_IT', 'absolute_tol': '-imp_atol', 'relative_tol': '-imp_rtol','step_tol':'-imp_stol'}
        for key, flag in tol_map.items():
            if key in t: flags[flag] = t[key]
    if 'pressure_solver' in solver_cfg:
        ps = solver_cfg['pressure_solver']
        if 'tolerance' in ps: flags['-poisson_tol'] = ps['tolerance']
        if 'multigrid' in ps:
            mg = ps['multigrid']
            mg_map = {'levels': '-mg_level', 'pre_sweeps': '-mg_pre_it', 'post_sweeps': '-mg_post_it'}
            for key, flag in mg_map.items():
                if key in mg: flags[flag] = mg[key]
            if 'semi_coarsening' in mg:
                sc = mg['semi_coarsening']
                if 'i' in sc: flags['-mg_i_semi'] = format_flag_value(sc['i'])
                if 'j' in sc: flags['-mg_j_semi'] = format_flag_value(sc['j'])
                if 'k' in sc: flags['-mg_k_semi'] = format_flag_value(sc['k'])
            if 'level_solvers' in mg:
                for level_name, settings in mg['level_solvers'].items():
                    level_num = level_name.split('_')[-1]
                    for key, value in settings.items():
                        flags[f"-ps_mg_levels_{level_num}_{key}"] = format_flag_value(value)
    if 'petsc_passthrough_options' in solver_cfg:
        passthrough = solver_cfg['petsc_passthrough_options']
        if passthrough:
            for key, value in passthrough.items():
                flags[key] = format_flag_value(value)
    return flags

def generate_solver_control_file(run_dir, run_id, configs, num_procs, monitor_files):
    """!
    @brief Generates the main .control file for the C-solver.
    @details Orchestrates the conversion of all YAML configurations (case, solver, monitor)
             into a single, machine-readable file of command-line flags.
    @param[in] run_dir The path to the main run directory.
    @param[in] run_id The unique identifier for the run.
    @param[in] configs A dictionary containing the parsed YAML data.
    @param[in] num_procs The number of MPI processes for the run.
    @param[in] monitor_files A dictionary containing paths to generated monitor files.
    @return The absolute path to the generated solver control file.
    """
    print("[INFO] Generating master solver control file...")
    case_cfg, solver_cfg, monitor_cfg = configs['case'], configs['solver'], configs['monitor']
    source_files = {'Case': configs['case_path'], 'Solver': configs['solver_path'], 'Monitor': configs['monitor_path']}
    
    control_lines = []
    try:
        props, run_ctrl = case_cfg['properties'], case_cfg['run_control']
        scales, fluid, ic = props['scaling'], props['fluid'], props['initial_conditions']
        L_ref, U_ref, rho, mu = float(scales['length_ref']), float(scales['velocity_ref']), float(fluid['density']), float(fluid['viscosity'])
        reynolds = (rho * U_ref * L_ref) / mu if mu != 0 else float('inf')
        dt_phys = float(run_ctrl['dt_physical'])
        T_ref = L_ref / U_ref if U_ref != 0 else float('inf')
        dt_nondim = dt_phys / T_ref if T_ref != float('inf') else 0.0
        u, v, w = float(ic['u_physical']), float(ic['v_physical']), float(ic['w_physical'])
        finit_map = {'Zero': 0, 'Constant': 1, 'Poiseuille': 2}
        finit_mode_str = ic.get('mode', 'Constant')
        finit_code = finit_map.get(finit_mode_str)
        if finit_code is None: raise ValueError(f"Unknown initial_conditions mode '{finit_mode_str}'.")
        print(f"  - Reynolds Number (Re) = {reynolds:.4f}")
        print(f"  - Non-Dimensional dt*  = {dt_nondim:.6f}")
        print(f"  - Field Initialization Mode: {finit_mode_str} (Code: {finit_code})")
        control_lines.extend([
            f"-start_step {run_ctrl['start_step']}", f"-totalsteps {run_ctrl['total_steps']}",
            f"-ren {reynolds}", f"-dt {dt_nondim}", f"-finit {finit_code}",
            f"-ucont_x {u/U_ref if U_ref!=0 else 0}", f"-ucont_y {v/U_ref if U_ref!=0 else 0}", f"-ucont_z {w}",
            f"-scaling_L_ref {L_ref}", f"-scaling_U_ref {U_ref}", f"-scaling_rho_ref {rho}"
        ])
    except (KeyError, TypeError, ZeroDivisionError, ValueError) as e:
        print(f"[FATAL] Error processing case.yml: {e}", file=sys.stderr)
        sys.exit(1)
        
    bcs_files = generate_multi_block_bcs(run_dir, run_id, case_cfg, source_files)
    control_lines.append(f"-bcs_files \"{','.join(bcs_files)}\"")

    # --- CORRECTED: Add paths for whitelist and profile files ---
    control_lines.append(f"-whitelist_config_file {monitor_files['whitelist']}")
    control_lines.append(f"-profile_config_file {monitor_files['profile']}")
    
    grid_cfg = case_cfg.get('grid', {})
    if grid_cfg.get('mode') == 'file':
        print("[INFO] Grid Mode: Using external file...")
        case_file_dir = os.path.dirname(configs['case_path'])
        source_grid = os.path.join(case_file_dir, grid_cfg['source_file'])
        nondim_grid_path = os.path.join(run_dir, "config", "grid.run")
        try:
            with open(source_grid, 'r') as f_in:
                header = [f_in.readline() for _ in range(3)]
                coords = np.loadtxt(f_in)
            coords_nondim = coords / L_ref if L_ref != 0 else coords
            with open(nondim_grid_path, 'w') as f_out:
                f_out.writelines(header)
                np.savetxt(f_out, coords_nondim, fmt='%.8e')
            print(f"[SUCCESS] Generated non-dimensional grid: {os.path.relpath(nondim_grid_path)}")
            control_lines.append(f"-grid_file {nondim_grid_path}")
        except Exception as e:
            print(f"[FATAL] Failed to process grid file '{source_grid}': {e}", file=sys.stderr)
            sys.exit(1)
    elif grid_cfg.get('mode') == 'programmatic_c':
        print("[INFO] Grid Mode: Programmatic C...")
        grid_settings = grid_cfg.get('programmatic_settings', {})
        control_lines.append("-grid")
        px, py, pz = grid_settings.get('da_processors_x'), grid_settings.get('da_processors_y'), grid_settings.get('da_processors_z')
        if num_procs > 1 and all(p is not None for p in [px, py, pz]):
            if isinstance(px, list):
                total_layout = sum(p_x * p_y * p_z for p_x, p_y, p_z in zip(px, py, pz))
            else:
                total_layout = px * py * pz
            if total_layout != num_procs:
                raise ValueError(f"Processor layout mismatch: product ({total_layout}) != processes ({num_procs}).")
            print(f"[INFO] Applying user-defined processor layout for {num_procs} processes.")
        else:
            if num_procs == 1: print("[INFO] Serial run, ignoring da_processors layout.")
            else: print("[INFO] Letting PETSc automatically determine processor layout.")
            for p_key in ['da_processors_x', 'da_processors_y', 'da_processors_z']:
                grid_settings.pop(p_key, None)
        for key, value in grid_settings.items(): control_lines.append(f"-{key} {format_flag_value(value)}")
    else:
        raise ValueError(f"Unknown or missing grid mode '{grid_cfg.get('mode')}' in case.yml.")
    
    parse_and_add_model_flags(case_cfg, control_lines)
    
    if 'solver_parameters' in case_cfg:
        params = case_cfg['solver_parameters']
        if params:
            for key, value in params.items():
                control_lines.append(f"{key} {format_flag_value(value)}")

    solver_flags = parse_solver_config(solver_cfg)
    for flag, value in solver_flags.items(): control_lines.append(f"{flag} {value}")
    
    io_cfg = monitor_cfg.get('io', {})
    if 'data_output_frequency' in io_cfg: control_lines.append(f"-tio {io_cfg['data_output_frequency']}")
    if 'particle_log_interval' in io_cfg: control_lines.append(f"-logfreq {io_cfg['particle_log_interval']}")
    if 'directories' in io_cfg:
        dirs = io_cfg['directories']
        if 'output' in dirs: control_lines.append(f"-output_dir {dirs['output']}")
        if 'restart' in dirs: control_lines.append(f"-restart_dir {dirs['restart']}")
        if 'log' in dirs: control_lines.append(f"-log_dir {dirs['log']}")
        
    final_content = generate_header(run_id, source_files) + "\n".join(control_lines)
    control_file_path = os.path.join(run_dir, "config", f"{run_id}.control")
    with open(control_file_path, "w") as f: f.write(final_content)
    print(f"[SUCCESS] Generated solver control file: {os.path.relpath(control_file_path)}")
    return os.path.abspath(control_file_path)

def generate_post_recipe_file(run_dir: str, run_id: str, post_cfg: dict, source_files: dict) -> str:
    """!
    @brief Generates a key=value config file (post.run) for the C post-processor.
    @details Translates the structured post-processing YAML into the specific flat
             key-value format required by the C executable, including complex,
             semicolon-separated pipeline strings.
    @param[in] run_dir The path to the main run directory.
    @param[in] run_id The unique identifier for the run.
    @param[in] post_cfg The parsed post-profile YAML configuration dictionary.
    @param[in] source_files A dictionary of source files for the header.
    @return The absolute path to the generated post.run recipe file.
    """
    print("[INFO] Generating post-processor recipe file (post.run)...")
    config_dir = os.path.join(run_dir, "config")
    post_recipe_path = os.path.join(config_dir, "post.run")
    
    lines = [generate_header(run_id, source_files)]
    c_config = {}

    # --- 1. Process Run Control ---
    rc = post_cfg.get('run_control', {})
    c_config['startTime'] = rc.get('start_step', 0)
    c_config['endTime'] = rc.get('end_step', 10)
    c_config['timeStep'] = rc.get('step_interval', 1)

    # --- 2. Process Global Operations ---
    eulerian_pipeline_parts = []
    if post_cfg.get('global_operations', {}).get('dimensionalize', False):
        eulerian_pipeline_parts.append('DimensionalizeAllLoadedFields')

    # --- 3. Build Eulerian Pipeline String ---
    for task in post_cfg.get('eulerian_pipeline', []):
        task_name = task.get('task')
        if task_name == 'q_criterion':
            eulerian_pipeline_parts.append('ComputeQCriterion')
        elif task_name == 'normalize_field':
            field = task.get('field', 'P')
            eulerian_pipeline_parts.append(f'NormalizeRelativeField:{field}')
            ref_point = task.get('reference_point', [1, 1, 1])
            c_config['reference_ip'] = ref_point[0]
            c_config['reference_jp'] = ref_point[1]
            c_config['reference_kp'] = ref_point[2]
        elif task_name == 'nodal_average':
            in_field = task.get('input_field')
            out_field = task.get('output_field')
            if in_field and out_field:
                eulerian_pipeline_parts.append(f'CellToNodeAverage:{in_field}>{out_field}')

    if eulerian_pipeline_parts:
        c_config['process_pipeline'] = ";".join(eulerian_pipeline_parts)

    # --- 4. Build Lagrangian Pipeline String ---
    lagrangian_pipeline_parts = []
    for task in post_cfg.get('lagrangian_pipeline', []):
        task_name = task.get('task')
        if task_name == 'specific_ke':
            in_field = task.get('input_field')
            out_field = task.get('output_field')
            if in_field and out_field:
                lagrangian_pipeline_parts.append(f'ComputeSpecificKE:{in_field}>{out_field}')
    
    if lagrangian_pipeline_parts:
        c_config['particle_pipeline'] = ";".join(lagrangian_pipeline_parts)

    # --- 5. Process I/O ---
    io = post_cfg.get('io', {})
    c_config['output_prefix'] = io.get('output_directory','viz')+'/'+io.get('output_filename_prefix', 'Field')
    c_config['particle_output_prefix'] = io.get('output_directory','viz')+'/'+io.get('particle_filename_prefix', 'Particle')
    c_config['output_particles'] = io.get('output_particles', False)
    c_config['particle_output_freq'] = io.get('particle_subsampling_frequency', 1)
    c_config['output_fields_instantaneous'] = ",".join(io.get('eulerian_fields', []))
    c_config['particle_fields_instantaneous'] = ",".join(io.get('particle_fields', []))

    # --- 6. Add Source Directory ---
    if 'source_data' in post_cfg and 'directory' in post_cfg['source_data']:
        c_config['source_directory'] = post_cfg['source_data']['directory']

    # --- 7. Write the final file ---
    for key, value in c_config.items():
        if value is not None and str(value) != "":
            lines.append(f"{key} = {value}")

    with open(post_recipe_path, "w") as f: f.write("\n".join(lines))
    print(f"[SUCCESS] Generated post-processor recipe: {os.path.relpath(post_recipe_path)}")
    return os.path.abspath(post_recipe_path)

def execute_command(command: list, run_dir: str, log_filename: str, monitor_cfg: dict = None):
    """!
    @brief Executes a command, streaming its output to the console and a log file.
    ...
    @param[in] monitor_cfg Optional. If provided, used to set LOG_LEVEL in a custom environment.
                           If None, the process inherits the parent's environment directly.
    """
    # Create the log directory if it doesn't exist.
    log_dir = os.path.join(run_dir, "logs")
    os.makedirs(log_dir, exist_ok=True)
    
    log_path = os.path.join(log_dir, log_filename)
    print(f"[INFO] Launching Command...\n  > {' '.join(command)}")
    print(f"       Log file: {os.path.relpath(log_path)}")
    print("-" * 60)

    # --- Environment Handling ---
    popen_kwargs = {
        "stdout": subprocess.PIPE, "stderr": subprocess.STDOUT,
        "cwd": run_dir, "bufsize": 1, "universal_newlines": True,
        "encoding": 'utf-8', "errors": 'replace'
    }

    if monitor_cfg:
        print("[INFO] Creating custom environment to set LOG_LEVEL.")
        run_env = os.environ.copy()
        verbosity = monitor_cfg.get('logging', {}).get('verbosity', 'INFO').upper()
        run_env['LOG_LEVEL'] = verbosity
        print(f"[INFO] Setting LOG_LEVEL={verbosity} for C executable.")
        popen_kwargs['env'] = run_env
    else:
        print("[INFO] Using inherited environment for process.")

    print("-" * 60)
    try:
        # Pass the constructed keyword arguments dictionary to Popen
        process = subprocess.Popen(command, **popen_kwargs)
        
        with open(log_path, "w") as log_file:
            for line in process.stdout:
                sys.stdout.write(line)
                log_file.write(line)
        process.wait()
        return_code = process.returncode
        print("-" * 60)
        if return_code == 0:
            print(f"[SUCCESS] Execution finished successfully.")
        else:
            print(f"[FATAL] Execution failed with exit code {return_code}. Check log: {os.path.relpath(log_path)}", file=sys.stderr)
            sys.exit(return_code)
    except FileNotFoundError:
        print(f"[FATAL] Command not found or is not executable: '{command[0]}'", file=sys.stderr)
        print("        Please check that the path is correct and the file has execute permissions.", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"[FATAL] An unexpected error occurred during execution: {e}", file=sys.stderr)
        sys.exit(1)

def run_workflow(args):
    """!
    @brief Main orchestrator for the 'run' command.
    @details This function manages the entire simulation workflow, including setting up
             run directories, generating configuration files, and executing the
             solver and/or post-processor stages.
    @param[in] args The command-line arguments parsed by argparse.
    """
    run_dir = None
    run_id = None
    
    # --- Stage 1: Solver (if requested) ---
    if args.solve:
        configs = {
            'case': read_yaml_file(args.case), 'case_path': args.case,
            'solver': read_yaml_file(args.solver), 'solver_path': args.solver,
            'monitor': read_yaml_file(args.monitor), 'monitor_path': args.monitor
        }
        case_name = os.path.splitext(os.path.basename(args.case))[0]
        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        run_id = f"{case_name}_{timestamp}"
        run_dir = os.path.abspath(os.path.join("runs", run_id))
        
        config_dir = os.path.join(run_dir, "config")
        for d in [config_dir, "logs", "results"]:
            os.makedirs(os.path.join(run_dir, d), exist_ok=True)
        print(f"[INFO] Created new self-contained run directory: {os.path.relpath(run_dir)}")
        
        shutil.copy(args.case, os.path.join(config_dir, "case.yml"))
        shutil.copy(args.solver, os.path.join(config_dir, "solver.yml"))
        shutil.copy(args.monitor, os.path.join(config_dir, "monitor.yml"))

        print("\n" + "="*25 + " SOLVER STAGE " + "="*25)
        
        source_files = {'Case': args.case, 'Solver': args.solver, 'Monitor': args.monitor}
        print("[INFO] Generating monitoring files (whitelist.run, profile.run)...")
        whitelist_path = generate_simple_list_file(run_dir, run_id, configs['monitor'], 'logging', 'enabled_functions', 'whitelist.run', source_files)
        profile_path = generate_simple_list_file(run_dir, run_id, configs['monitor'], 'profiling', 'critical_functions', 'profile.run', source_files)
        
        monitor_files = {'whitelist': whitelist_path, 'profile': profile_path}
        control_file = generate_solver_control_file(run_dir, run_id, configs, args.num_procs, monitor_files)
        
        solver_exe = os.path.join(BIN_DIR, "picsolver")
        command = [solver_exe, "-control_file", control_file]
        if args.num_procs > 1:
            command = ["mpiexec", "-n", str(args.num_procs)] + command
        execute_command(command, run_dir, f"{run_id}_solver.log", configs['monitor'])

    # --- Stage 2: Post-Processing (if requested) ---
    if args.post_process:
            if args.run_dir:
                run_dir = os.path.abspath(args.run_dir)
                if not os.path.isdir(run_dir):
                    print(f"[FATAL] Specified run directory not found: {run_dir}", file=sys.stderr)
                    sys.exit(1)
                print(f"[INFO] Operating on existing run directory: {os.path.relpath(run_dir)}")
                run_id = os.path.basename(run_dir)
            elif not args.solve:
                main_parser.error("--post-process requires --run-dir when not used with --solve.")
            
            print("\n" + "="*20 + " POST-PROCESSING STAGE " + "="*20)
            config_dir = os.path.join(run_dir, "config")
            
            # --- Find necessary config files by inspecting their content ---
            all_yml_files = glob.glob(os.path.join(config_dir, "*.yml"))
            case_path, monitor_path = None, None
            for f_path in all_yml_files:
                try:
                    content = read_yaml_file(f_path)
                    if not isinstance(content, dict): continue
                    if 'models' in content and 'boundary_conditions' in content: case_path = f_path
                    elif 'io' in content and 'logging' in content: monitor_path = f_path
                except Exception as e:
                    print(f"[WARNING] Could not parse or inspect '{f_path}': {e}", file=sys.stderr)
                    continue
            try:
                solver_control_path = glob.glob(os.path.join(config_dir, "*.control"))[0]
            except IndexError: solver_control_path = None

            if not all([case_path, monitor_path, solver_control_path]):
                print(f"[FATAL] Could not automatically identify required config files in {config_dir}", file=sys.stderr)
                if not case_path: print("         - No 'case' file found (expected to contain 'models' and 'boundary_conditions').", file=sys.stderr)
                if not monitor_path: print("         - No 'monitor' file found (expected to contain 'io' and 'logging').", file=sys.stderr)
                if not solver_control_path: print("         - No '.control' file found.", file=sys.stderr)
                sys.exit(1)
            print(f"[INFO] Auto-identified Case file:    {os.path.basename(case_path)}")
            print(f"[INFO] Auto-identified Monitor file: {os.path.basename(monitor_path)}")

            case_cfg = read_yaml_file(case_path)
            monitor_cfg = read_yaml_file(monitor_path)
            post_cfg = read_yaml_file(args.post)

            # --- Resolve Source Data Directory & Verify Existence ---
            solver_output_dir_rel = monitor_cfg.get('io', {}).get('directories', {}).get('output', 'results')
            solver_output_dir_abs = os.path.join(run_dir, solver_output_dir_rel)
            source_dir_template = post_cfg.get('source_data', {}).get('directory', '<solver_output_dir>')
            if source_dir_template == '<solver_output_dir>':
                resolved_source_dir = solver_output_dir_abs
                print(f"[INFO] Post-processor will read data from solver's output directory: {os.path.relpath(resolved_source_dir)}")
            else:
                resolved_source_dir = os.path.abspath(os.path.join(run_dir, source_dir_template))
                print(f"[INFO] Post-processor will read from user-specified directory: {os.path.relpath(resolved_source_dir)}")
            if not os.path.isdir(resolved_source_dir) or not os.listdir(resolved_source_dir):
                 print(f"[FATAL] Source data directory for post-processing not found or is empty: {os.path.relpath(resolved_source_dir)}", file=sys.stderr)
                 sys.exit(1)
            print("[SUCCESS] Source data directory verified.")
            if 'source_data' not in post_cfg: post_cfg['source_data'] = {}
            post_cfg['source_data']['directory'] = resolved_source_dir
            
            # --- Explicit Output Directory and Prefix Logic ---
            post_io_cfg = post_cfg.get('io', {})
            try:
                output_dir_rel = post_io_cfg['output_directory']
                output_prefix = post_io_cfg['output_filename_prefix']
            except KeyError as e:
                print(f"[FATAL] Missing required key '{e.args[0]}' in the 'io' section of {args.post}", file=sys.stderr)
                sys.exit(1)
            output_dir_abs = os.path.abspath(os.path.join(run_dir, output_dir_rel))
            os.makedirs(output_dir_abs, exist_ok=True)
            print(f"[INFO] Post-processor will write output to: {os.path.relpath(output_dir_abs)}")
            
            # --- Generate the post-processing recipe file (post.run) ---
            source_files_post = {'Case': case_path, 'Post-Profile': args.post}
            post_recipe_file = generate_post_recipe_file(run_dir, run_id, post_cfg, source_files_post)
            
            # --- Build and execute the post-processor command ---
            post_exe = os.path.join(BIN_DIR, "postprocessor")
            command = [
                post_exe,
                "-control_file", solver_control_path,
                "-postprocessing_config_file", post_recipe_file
            ]
            if args.num_procs > 1:
                command = ["mpiexec", "-n", str(args.num_procs)] + command
            
            execute_command(command, run_dir, f"{run_id}_{output_prefix}.log", monitor_cfg)

def init_case(args):
    """!
    @brief Implements the 'init' command.
    @details Creates a new case study directory by copying a template. It can then
             either create relative symbolic links to the project's executables
             (default) or create a full copy of them for a self-contained study.
    @param[in] args The command-line arguments parsed by argparse.
    """
    template_path = os.path.join(PROJECT_ROOT, "examples", args.template_name)
    # The destination path is relative to the current working directory.
    dest_path = os.path.abspath(os.path.join(os.getcwd(), args.dest_name if args.dest_name else args.template_name))

    if not os.path.isdir(template_path):
        print(f"[FATAL] Case template '{args.template_name}' not found at '{template_path}'", file=sys.stderr)
        sys.exit(1)
    if os.path.exists(dest_path):
        print(f"[FATAL] Destination directory '{dest_path}' already exists.", file=sys.stderr)
        sys.exit(1)

    print(f"[INFO] Initializing new case '{os.path.basename(dest_path)}' from template '{args.template_name}'...")
    
    shutil.copytree(template_path, dest_path)
    print(f"[SUCCESS] Copied template files to: {dest_path}")
    
    if args.copy_binaries:
        print("[INFO] Copying project binaries for a self-contained case...")
    else:
        print("[INFO] Creating symbolic links to project binaries...")
        
    main_bin_dir = os.path.join(PROJECT_ROOT, "bin")
    
    try:
        # We only want to copy/link the actual executables, not the orchestrator script itself.
        binaries = [f for f in os.listdir(main_bin_dir) if f != 'pic.flow' and os.path.isfile(os.path.join(main_bin_dir, f))]
        if not binaries:
            print("[WARNING] Main project bin/ directory contains no executables. Nothing to link or copy.", file=sys.stderr)
            return

        for binary_name in binaries:
            source_path_abs = os.path.abspath(os.path.join(main_bin_dir, binary_name))
            dest_file_path = os.path.join(dest_path, binary_name)
            
            if args.copy_binaries:
                # Use copy2 to preserve permissions and metadata
                shutil.copy2(source_path_abs, dest_file_path)
                print(f"  - Copied '{binary_name}'")
            else:
                relative_source_path = os.path.relpath(source_path_abs, start=dest_path)
                os.symlink(relative_source_path, dest_file_path)
                print(f"  - Linked '{binary_name}'")
            
        print("[SUCCESS] Binaries are now available in your study directory.")
        print("          You can now 'cd' into your study and run commands locally (e.g., './picsolver ...').")

    except Exception as e:
        print(f"[ERROR] Failed to process binaries: {e}", file=sys.stderr)
        print("        Your case files were copied, but you will need to run commands from the project root.", file=sys.stderr)

def build_project(args):
    """!
    @brief Implements the 'build' command.
    @details Executes the top-level build.sh script, passing through any
             additional arguments directly to 'make'. This allows for building,
             cleaning, and other Makefile targets via the orchestrator.
    @param[in] args The command-line arguments parsed by argparse.
    """

    print("\n" + "="*27 + " BUILD STAGE " + "="*27)
    build_script_path = os.path.join(PROJECT_ROOT, "build.sh")

    if not os.path.isfile(build_script_path):
        print(f"[FATAL] Build script not found at expected location: {build_script_path}", file=sys.stderr)
        print("        Please ensure 'build.sh' exists in the project root directory.", file=sys.stderr)
        sys.exit(1)

    # The command is the script itself, plus any passthrough arguments for make.
   # command = [build_script_path] + args.make_args
    command = ['/bin/bash', build_script_path] + args.make_args
    # For the build process, we don't have a monitor.yml, so we pass an empty
    # dict to execute_command. The command should be run in the project root.
    execute_command(command, PROJECT_ROOT, "build.log", {})
   



# ==============================================================================
# MAIN COMMAND-LINE INTERFACE PARSER
# ==============================================================================

if __name__ == "__main__":
    main_parser = argparse.ArgumentParser(
        description="pic.flow: A comprehensive conductor for the PIC-Flow CFD simulation platform.",
        formatter_class=argparse.RawTextHelpFormatter,
    )
    subparsers = main_parser.add_subparsers(dest='command', required=True, help="Available commands")

    # --- 'run' command ---
    p_run = subparsers.add_parser('run', help="Execute a simulation workflow (solve and/or post-process).")
    run_group = p_run.add_argument_group('stages')
    run_group.add_argument('--solve', action='store_true', help="Execute the solver stage (creates a new run directory).")
    run_group.add_argument('--post-process', action='store_true', help="Execute the post-processing stage on a run directory.")
    
    solver_group = p_run.add_argument_group('solver inputs (required for --solve)')
    solver_group.add_argument('--case', help="Path to the case definition file (e.g., case.yml).")
    solver_group.add_argument('--solver', help="Path to the solver settings profile (e.g., solver.yml).")
    solver_group.add_argument('--monitor', help="Path to the monitoring and I/O profile (e.g., monitor.yml).")
    
    post_group = p_run.add_argument_group('post-processor inputs (required for --post-process)')
    post_group.add_argument('--run-dir', help="Path to an existing run directory to post-process.\n(Not needed if running with --solve in the same command).")
    post_group.add_argument('--post', help="Path to the post-processing recipe file (e.g., post.yml).")
    
    p_run.add_argument('-n', '--num-procs', type=int, default=1, help="Number of MPI processes for either stage.")

# --- 'init' command ---
    p_init = subparsers.add_parser('init', help="Initialize a new case study directory from a template.")
    p_init.add_argument('template_name', help="Name of the case template directory to copy (e.g., 'laminar_pipe').")
    p_init.add_argument('--dest', dest='dest_name', 
                        help="Optional name for the new directory. Defaults to the template name.\n"
                             "Path is relative to your current working directory.")
    p_init.add_argument('--copy-binaries', action='store_true',
                        help="Copy executables into the case directory instead of symlinking.\n"
                             "This creates a fully portable, self-contained case study.")

    # --- 'build' command ---
    p_build = subparsers.add_parser('build', help="Build project executables using the Makefile.",
                                    description="Calls the project's build.sh script. Any arguments provided\n"
                                                "after 'build' will be passed directly to 'make'.\n\n"
                                                "Examples:\n"
                                                "  pic-flow build                  (Runs 'make all')\n"
                                                "  pic-flow build clean-project    (Runs 'make clean-project')\n"
                                                "  pic-flow build SYSTEM=cluster   (Runs 'make SYSTEM=cluster')\n"
                                                "  pic-flow build postprocessor    (Runs 'make postprocessor')")
    p_build.add_argument('make_args', nargs=argparse.REMAINDER,
                         help="Arguments to pass directly to the make command (e.g., 'clean-project').")

    args = main_parser.parse_args()

    if args.command == 'run':
        if not args.solve and not args.post_process:
             main_parser.error("At least one stage (--solve or --post-process) must be selected.")
        if args.solve and (not args.case or not args.solver or not args.monitor):
            main_parser.error("--solve requires --case, --solver, and --monitor.")
        if args.post_process and not args.post:
            main_parser.error("--post-process requires a --post profile.")
        run_workflow(args)
    elif args.command == 'init':
        init_case(args)
    elif args.command == 'build':
        build_project(args)
    else:
        main_parser.print_help()